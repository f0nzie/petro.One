# Test that we can write and read HTML files generated by the basic functions
# write_html() and read_html() using the OnePetro URL parameters start and rows.
# Each page is read afterwards with onepetro_page_to_dataframe()

library(testthat)

skip_on_cran()
skip_on_travis()


context("paper count for search matches number above")
test_that("", {
    # Specifying the url for desired website to be scrapped
    # this URL is generated when we enter a standard search for "neural networks"
    url <- "https://www.onepetro.org/search?q=neural+networks&peer_reviewed=&published_between=&from_year=&to_year=&"
    result <- get_papers_count(url)
    expect_equal(result, 3788)
})

context("write HTML test pages")
test_that("pages #1 write/read", {
    # Specifying the url for desired website to be scrapped
    url <- "https://www.onepetro.org/search?start=0&q=neural+networks&from_year=&peer_reviewed=&published_between=&rows=1000&to_year="

    # write to HTML page
    url_page <- "neural_network-s0000r1000.html"
    webpage <- read_html(url)
    xml2::write_html(webpage, file = url_page)

    # read from HTML page
    result <- onepetro_page_to_dataframe(url_page)
    expect_equal(dim(result), c(1000, 6))
})

test_that("page #2 write/read", {
    # Specifying the url for desired website to be scrapped
    url <- "https://www.onepetro.org/search?start=1000&q=neural+networks&from_year=&peer_reviewed=&published_between=&rows=1000&to_year="

    # write to HTML page
    url_page <- "neural_network-s1000r1000.html"
    webpage <- read_html(url)
    xml2::write_html(webpage, file = url_page)

    # read from HTML page
    result <- onepetro_page_to_dataframe(url_page)
    expect_equal(dim(result), c(1000, 6))
})

test_that("page #3 write/read", {
    # Specifying the url for desired website to be scrapped
    url <- "https://www.onepetro.org/search?start=2000&q=neural+networks&from_year=&peer_reviewed=&published_between=&rows=1000&to_year="

    # write to HTML page
    url_page <- "neural_network-s2000r1000.html"
    webpage <- read_html(url)
    xml2::write_html(webpage, file = url_page)

    # read from HTML page
    result <- onepetro_page_to_dataframe(url_page)
    expect_equal(dim(result), c(1000, 6))
})

test_that("page #4 write/read", {
    # Specifying the url for desired website to be scrapped
    url <- "https://www.onepetro.org/search?start=3000&q=neural+networks&from_year=&peer_reviewed=&published_between=&rows=1000&to_year="

    # write to HTML page
    url_page <- "neural_network-s3000r1000.html"
    webpage <- read_html(url)
    xml2::write_html(webpage, file = url_page)

    # read from HTML page
    result <- onepetro_page_to_dataframe(url_page)
    expect_equal(dim(result), c(788, 6))
})


context("HTML saved pages match paper count")
test_that("papers count is correct", {
    # something interesting to note is that we did the manual split of the
    # web pages, then the paper count at each of the HTML files is the same
    # since we will getting the total count of the search not the partial
    # count per page, which is 1000 for 1st and 2nd pages
    pc1 <- get_papers_count("neural_network-s0000r1000.html")
    pc2 <- get_papers_count("neural_network-s1000r1000.html")
    pc3 <- get_papers_count("neural_network-s2000r1000.html")
    pc4 <- get_papers_count("neural_network-s3000r1000.html")
    expect_equal(c(pc1, pc2, pc3, pc4), c(3788, 3788, 3788, 3788))
})



context("webpage neural_network-s0000r1000.html")

test_that("webpage neural_network-s0000r1000.html matches", {
    url_page <- "neural_network-s0000r1000.html"
    webpage <- xml2::read_html(url_page)

    result <- petro.One:::get_dc_type(webpage)
    expect_equal(length(result), 1000)

    result <- get_book_title(webpage)
    expect_equal(length(result), 1000)

    result <- get_paper_id(webpage)
    expect_equal(length(result), 1000)

    result <- get_authors(webpage)
    expect_equal(length(result), 1)

    result <- get_year(webpage)
    expect_equal(length(result), 1)

    result <- get_source(webpage)
    expect_equal(length(result), 1000)
})

context("webpage neural_network-s1000r1000.html")

test_that("webpage neural_network-s1000r1000.html matches", {
    url_page <- "neural_network-s1000r1000.html"
    webpage <- xml2::read_html(url_page)

    result <- petro.One:::get_dc_type(webpage)
    expect_equal(length(result), 1000)

    result <- get_book_title(webpage)
    expect_equal(length(result), 1000)

    result <- get_paper_id(webpage)
    expect_equal(length(result), 1000)

    result <- get_authors(webpage)
    expect_equal(length(result), 1)

    result <- get_year(webpage)
    expect_equal(length(result), 1)

    result <- get_source(webpage)
    expect_equal(length(result), 1000)
})

context("webpage neural_network-s2000r1000.html")

test_that("neural_network-s2000r1000.html matches", {
    url_page <- "neural_network-s2000r1000.html"
    webpage <- xml2::read_html(url_page)

    result <- petro.One:::get_dc_type(webpage)
    expect_equal(length(result), 1000)

    result <- get_book_title(webpage)
    expect_equal(length(result), 1000)

    result <- get_paper_id(webpage)
    expect_equal(length(result), 1000)

    result <- get_authors(webpage)
    expect_equal(length(result), 1)

    result <- get_year(webpage)
    expect_equal(length(result), 1)

    result <- get_source(webpage)
    expect_equal(length(result), 1000)
})


context("webpage neural_network-s3000r1000.html")

test_that("webpage neural_network-s3000r1000.html matches", {
    url_page <- "neural_network-s3000r1000.html"
    webpage <- xml2::read_html(url_page)

    result <- petro.One:::get_dc_type(webpage)
    expect_equal(length(result), 788)

    result <- get_book_title(webpage)
    expect_equal(length(result), 788)

    result <- get_paper_id(webpage)
    expect_equal(length(result), 788)

    result <- get_authors(webpage)
    expect_equal(length(result), 1)

    result <- get_year(webpage)
    expect_equal(length(result), 1)

    result <- get_source(webpage)
    expect_equal(length(result), 788)
})
